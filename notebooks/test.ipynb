{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7688e2d2-39de-4c46-aa41-6ee287a036b4",
   "metadata": {},
   "source": [
    "# Movie Recomendation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e48a99-2607-4be5-98ca-aa69368180cf",
   "metadata": {},
   "source": [
    "It is a content based recomendation system that suggest or recommend moview to the user based on short description, keywords, title and actor names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16daf03-4e43-48c9-bd02-7d102b7bfb49",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c81f71f-75e7-4567-8d5d-586590d502f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed545bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\palma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\palma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download stopwords and wordnet if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ea7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0f795e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Movie recommendation system'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6250b-cfd3-4ce2-aedc-82222f0889e1",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bebdbbb-cf10-4840-be90-e0a280fcd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "df_credits = pd.read_csv('artifacts/raw/tmdb_5000_credits.csv')\n",
    "df_movies = pd.read_csv('artifacts/raw/tmdb_5000_movies.csv')\n",
    "\n",
    "df = df_movies.merge(df_credits, left_on='id', right_on='movie_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73f7a14e-03a1-41ce-a25a-5b88bd334002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['budget', 'genres', 'homepage', 'id', 'keywords', 'original_language',\n",
       "       'original_title', 'overview', 'popularity', 'production_companies',\n",
       "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
       "       'spoken_languages', 'status', 'tagline', 'title_x', 'vote_average',\n",
       "       'vote_count', 'movie_id', 'title_y', 'cast', 'crew'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb75d1-77b3-43fa-bf05-1966d80dc779",
   "metadata": {},
   "source": [
    "Important columns\n",
    "\n",
    "1. id\n",
    "2. title\n",
    "3. overview\n",
    "4. genres\n",
    "5. cast\n",
    "6. crew\n",
    "7. keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a59d8ec-a148-47cd-a281-af52613b7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "\"id\",\n",
    "\"original_title\",\n",
    "\"overview\",\n",
    "\"genres\",\n",
    "\"cast\",\n",
    "\"crew\",\n",
    "\"keywords\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69146d5-8925-4e23-a67a-bd2f9121ed53",
   "metadata": {},
   "source": [
    "### Data cleaning and formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "334dca95-b360-4045-b3fc-bb04e523a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format genres column\n",
    "def convert_to_list(row):\n",
    "    l = []\n",
    "    for i in ast.literal_eval(row):\n",
    "        l.append(i['name'])\n",
    "    return ' '.join(l)\n",
    "df['genres'] = df['genres'].apply(convert_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fdd9b65-775b-4cb9-9fbe-efd6dca319d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format cast column\n",
    "def convert_to_list_cast(row):\n",
    "    l= []\n",
    "    flag = 0\n",
    "    for i in ast.literal_eval(row):\n",
    "        if flag == 5:\n",
    "            break\n",
    "        flag+=1\n",
    "        l.append(i['character'])\n",
    "        l.append(i['name'])\n",
    "    return ' '.join(l)\n",
    "df['cast'] = df['cast'].apply(convert_to_list_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b21a335d-f3ed-459c-8fc1-d3c5c0f573e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast.literal_eval(df['crew'][0])\n",
    "# format crew column\n",
    "\n",
    "def convert_to_list_crew(row):\n",
    "    l = []\n",
    "    for i in ast.literal_eval(row):\n",
    "        if i['job'] == 'Director':\n",
    "            l.append(i['name'])\n",
    "            break\n",
    "    return ' '.join(l)\n",
    "df['crew'] = df['crew'].apply(convert_to_list_crew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f6f18-cbf1-485c-9105-68e4734786d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "176de539-1e31-463c-98a6-4eccd5fcd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kew=yword column formating\n",
    "df['keywords'] = df['keywords'].apply(convert_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e2a9f81-08dc-413f-b880-06de7ad45f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'original_title':'title'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7e4956-fe20-4fd2-b4fd-ba2251847cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] = df['title'] + df['overview'] + df['genres'] + df['cast'] + df['crew'] + df['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a8f26f3-e0b5-44d5-85f5-a1e77eee3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[['id', 'title', 'tags']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77adef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('artifacts/raw/movies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc2ff4-ef0a-4253-8074-64926e96e048",
   "metadata": {},
   "source": [
    "### Basic text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1c752f0e-119c-4f84-b8cd-3b78e4d24111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BasicTextPreprocessing:  \n",
    "    def __init__(self):  \n",
    "        pass \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):  \n",
    "        \n",
    "        def keep_text_only(input_string):   \n",
    "            result = re.findall(r'[a-zA-Z\\s]+', input_string)   \n",
    "            return ''.join(result).strip()    \n",
    "        \n",
    "        def remove_urls(text):    \n",
    "            if isinstance(text, str):  # Check if the text is a string  \n",
    "                url_pattern = r'\\b(?:https?:\\/\\/)?(?:www\\.)?[a-zA-Z0-9-]+\\.[a-zA-Z]{2,}(?:\\/[^\\s]*)?\\b'    \n",
    "                cleaned_text = re.sub(url_pattern, '', text)   \n",
    "                return ' '.join(cleaned_text.split())  \n",
    "            return text  # Return as is for non-string inputs  \n",
    "            \n",
    "        def remove_punctuation(row):   \n",
    "            return row.translate(str.maketrans('', '', string.punctuation)) \n",
    "\n",
    "        def spell_correction(row):\n",
    "            l = []\n",
    "            for i in row.split():\n",
    "                l.append(str(TextBlob(i).correct()))\n",
    "            return ' '.join(l)\n",
    "            \n",
    "        def remove_stopwords(text):\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            return ' '.join([i for i in text.split() if i.lower() not in stop_words])\n",
    "\n",
    "        def lemmatization(row):  \n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            l = [lemmatizer.lemmatize(i) for i in row.split()]  \n",
    "            return ' '.join(l)\n",
    "            \n",
    "        X = X.str.lower()  \n",
    "        X = X.apply(keep_text_only)  \n",
    "        X = X.apply(remove_urls)  \n",
    "        X = X.apply(remove_punctuation)    \n",
    "        X = X.apply(remove_stopwords)  \n",
    "        X = X.apply(lemmatization)\n",
    "        return X.values\n",
    "        \n",
    "class CosineSimilarity:\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return cosine_similarity(X)\n",
    "        \n",
    "# Creating pipeline\n",
    "tokenizer = Pipeline(steps=[\n",
    "    ('basic preprocessing', BasicTextPreprocessing()),\n",
    "    ('count vectorization', CountVectorizer(max_features=10000)),\n",
    "    ('tf-idf', TfidfTransformer())\n",
    "    # ('cosign similarity', CosineSimilarity())\n",
    "])\n",
    "\n",
    "# Assuming df is defined AttributeErrorand 'tags' is a column\n",
    "similarity = tokenizer.fit_transform(new_df['tags']).toarray() # Fixed for Series input\n",
    "\n",
    "def recommend(text):\n",
    "    a = tokenizer.transform(pd.Series([text]))\n",
    "    distances = np.dot(similarity, a.T).ravel()/(np.linalg.norm(similarity, axis=1)*(np.linalg.norm(a)))\n",
    "    top_movie_index = sorted(list(enumerate(distances)), reverse=True, key=lambda x: x[1])[1:6]\n",
    "    return top_movie_index\n",
    "recommend('a horror movie with love story and romance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5eeb9-4650-4fa4-8bb0-edec1afda49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(text):\n",
    "    a = tokenizer.transform(pd.Series([text]))\n",
    "    distances = np.dot(similarity, a.T).ravel()/(np.linalg.norm(similarity, axis=1)*(np.linalg.norm(a)))\n",
    "    top_movie_index = sorted(list(enumerate(distances)), reverse=True, key=lambda x: x[1])[1:6]\n",
    "    return top_movie_index\n",
    "recommend('a horror movie with love story and romance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d355584f-4fe6-43a7-bde1-abcd9a0c07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'a horror love story movie'\n",
    "vector = tokenizer.transform(pd.Series([text]))\n",
    "a = vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "22fb463d-2936-4555-80d5-30a90a8fe2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4800, 10000), (1, 10000))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.shape, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7586406b-f53a-4075-a643-baa2b55835be",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.dot(similarity, a.T).ravel()/(np.linalg.norm(similarity, axis=1)*(np.linalg.norm(a)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
